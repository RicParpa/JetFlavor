{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RicParpa/JetFlavor/blob/main/JetFlavor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMblbs6y4EMF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfa5ffe6-a365-4b5a-e418-894120c44a36"
      },
      "source": [
        "!pip install pandas==1.2.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pandas==1.2.5\n",
            "  Downloading pandas-1.2.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (9.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.7 MB 21.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.2.5) (2022.6)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from pandas==1.2.5) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.2.5) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas==1.2.5) (1.15.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "Successfully installed pandas-1.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qrVGCqGLMFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26c6d78f-0d13-46a5-c6aa-959f58fe0312"
      },
      "source": [
        "!rm -rf dataset.json\n",
        "!rm -rf dataset.json.gz\n",
        "!wget http://mlphysics.ics.uci.edu/data/hb_jet_flavor_2016/dataset.json.gz\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-16 17:19:31--  http://mlphysics.ics.uci.edu/data/hb_jet_flavor_2016/dataset.json.gz\n",
            "Resolving mlphysics.ics.uci.edu (mlphysics.ics.uci.edu)... 128.195.1.86\n",
            "Connecting to mlphysics.ics.uci.edu (mlphysics.ics.uci.edu)|128.195.1.86|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2386808363 (2.2G) [application/x-gzip]\n",
            "Saving to: ‘dataset.json.gz’\n",
            "\n",
            "dataset.json.gz     100%[===================>]   2.22G  15.2MB/s    in 2m 21s  \n",
            "\n",
            "2022-12-16 17:21:52 (16.1 MB/s) - ‘dataset.json.gz’ saved [2386808363/2386808363]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGHYUQfSqWE_"
      },
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.image as mpimg\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH6hB2eQN1Tn"
      },
      "source": [
        "Leggiamo le prime 5 colonne del dataset 500000 righe alla volta e le uniamo in un dataframe di 2 milioni di righe (eventi)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr7J_9BvipYO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ef002fe-8ec1-47c1-b17b-2f21cb3f3c5d"
      },
      "source": [
        "chunks = pd.read_json('dataset.json.gz', lines = True, chunksize=500000)\n",
        "\n",
        "def concatenatore (ck):\n",
        "  i=0\n",
        "  tensor=[]\n",
        "  for k in ck:\n",
        "    if i>3:\n",
        "      break\n",
        "    k.drop(k.columns[[5]], axis = 1, inplace = True)\n",
        "    tensor.append(k)\n",
        "    i = i+1\n",
        "  return tensor;\n",
        "\n",
        "tensor = concatenatore(chunks)\n",
        "tensor = pd.concat(tensor)\n",
        "\n",
        "df = tensor\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               0         1  2  \\\n",
            "0        47.8712  1.893250  5   \n",
            "1        34.9703  0.609755  5   \n",
            "2        26.5706 -0.530268  5   \n",
            "3        36.9246 -2.074420  5   \n",
            "4        28.9667  1.512090  5   \n",
            "...          ...       ... ..   \n",
            "1999995  35.1177  0.252782  5   \n",
            "1999996  34.8303 -0.808358  5   \n",
            "1999997  29.4848 -0.840796  5   \n",
            "1999998  25.7466 -0.865773  5   \n",
            "1999999  28.9731 -2.326480  5   \n",
            "\n",
            "                                                         3  \\\n",
            "0        [21.1765, 8.36586, 29.0269, 4.149, 5, 2.68624e...   \n",
            "1        [5.0077, 3.03583, 2.00754, 0.32992099999999996...   \n",
            "2        [0.8078839999999999, 0.6305649999999999, 1.044...   \n",
            "3        [16.5617, 10.357, 17.5166, 30.4992, 4, 4.35319...   \n",
            "4        [18.0325, 16.1445, 3.29359, 9.00347, 4, 3.3143...   \n",
            "...                                                    ...   \n",
            "1999995  [2.11239, 1.33285, 0.441166, 0.217754, 2, 0.00...   \n",
            "1999996  [0.824012, 0.41628000000000004, 1.37785, 0.760...   \n",
            "1999997  [1.45084, 1.2525, 0.770663, 1.96815, 1, 0.0006...   \n",
            "1999998  [3.99771, -inf, 1.21638, -inf, 2, 6.86975e-05,...   \n",
            "1999999  [6.98399, 1.9517700000000002, 2.13684, 8.2088,...   \n",
            "\n",
            "                                                         4  \n",
            "0        [32.8902, 3, 7, 0.16244799999999998, 1.59408, ...  \n",
            "1        [25.1935, 1, 3, 0.122343, 2.23962, 0.930326999...  \n",
            "2                              [-1, -1, -1, inf, -1, -inf]  \n",
            "3            [73.6335, 1, 2, 0.259107, 0.755648, 0.594634]  \n",
            "4              [293.79, 1, 4, 0.0871323, 1.40008, 1.00014]  \n",
            "...                                                    ...  \n",
            "1999995  [11.3646, 1, 2, 1.05865, 1.40955, 0.4634169999...  \n",
            "1999996                        [-1, -1, -1, inf, -1, -inf]  \n",
            "1999997  [19.5591, 1, 2, 0.21088, 0.9837539999999999, 0...  \n",
            "1999998  [16.9286, 1, 2, 0.0895229, 0.888597, 0.9558249...  \n",
            "1999999      [53.6426, 1, 3, 0.0810526, 1.49895, 0.805407]  \n",
            "\n",
            "[2000000 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24xc-w1COOYj"
      },
      "source": [
        "Dividiamo il dataset in eventi di classe 1 (flavor = 5) ed eventi di classe 2 (flavor = 0 e flavor = 4). Poiché le due configurazioni sono molto sbilanciate, tagliamo la classe più numerosa allo stesso numero di eventi di quella meno numerosa.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dF62Trg0GaTV",
        "outputId": "266e6bf2-ece5-4f9d-a144-c2970176b2ec"
      },
      "source": [
        "cond1 = df[2] == 5\n",
        "dataset_classe1 = df[cond1].values\n",
        "#dataset_classe1\n",
        "\n",
        "cond2 = df[2] != 5\n",
        "dataset_classe2 = df[cond2].values\n",
        "#dataset_classe2\n",
        "\n",
        "print(dataset_classe1.shape, dataset_classe2.shape)\n",
        "print(len(dataset_classe2))\n",
        "\n",
        "dataset_classe1 = dataset_classe1[0:len(dataset_classe2)][:]\n",
        "print(dataset_classe1.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1828750, 5) (171250, 5)\n",
            "171250\n",
            "(171250, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQwSCzLKOoPC"
      },
      "source": [
        "Uniamo i dataset delle due classi e mescoliamo le righe in maniera casuale in modo da avere un dataset omogeneo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LR7YaksXMVog",
        "outputId": "168317d1-4462-41ac-f5e4-a7fadab2698b"
      },
      "source": [
        "dataset = np.concatenate((dataset_classe1,dataset_classe2), axis = 0)\n",
        "print(dataset.shape)\n",
        "np.random.shuffle(dataset)\n",
        "np.random.seed(1234)\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(342500, 5)\n",
            "[[24.8372 2.14779 5\n",
            "  list([40.0829, 38.9911, 42.8764, 17.6421, 4, 3.44542e-17, 0.144208, 0.252595])\n",
            "  list([76.4005, 1, 2, 0.38647299999999996, 0.319795, 0.18357199999999999])]\n",
            " [24.4613 1.8719800000000002 0\n",
            "  list([0.9655239999999999, '-inf', 0.542345, '-inf', 0, 0.10756099999999999, 0.0294951, 0.0981485])\n",
            "  list([-1, -1, -1, 'inf', -1, '-inf'])]\n",
            " [28.2742 -1.06965 5\n",
            "  list([21.8931, 13.5264, 3.95583, 2.00677, 6, 1.0714600000000003e-15, 0.095354, 0.138671])\n",
            "  list([88.4217, 2, 5, 0.113212, 1.98228, 0.6210519999999999])]\n",
            " ...\n",
            " [25.7177 0.421292 5\n",
            "  list([4.02657, 0.136679, 0.0224637, 0.47055, 2, 0.0007335340000000001, 0.165536, 0.10527])\n",
            "  list([-1, -1, -1, 'inf', -1, '-inf'])]\n",
            " [33.7001 -0.44517799999999996 5\n",
            "  list([1.50458, '-inf', 1.53197, '-inf', 1, 0.000434408, 0.106206, 0.0702319])\n",
            "  list([35.2886, 1, 2, 0.11746000000000001, 1.42181, 0.973598])]\n",
            " [26.0728 -2.4862 0\n",
            "  list(['-inf', '-inf', '-inf', '-inf', 0, 0.057028699999999995, 0.177813, 0.143865])\n",
            "  list([-1, -1, -1, 'inf', -1, '-inf'])]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spbzp46xO1d9"
      },
      "source": [
        "Adesso creiamo l'array con le labels del nuovo dataset bilanciato."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpYw5HjsA4_h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59ca51fb-d3d1-44f0-ad61-2dd14b054abb"
      },
      "source": [
        "labels = np.zeros(len(dataset))\n",
        "classe1 = 0\n",
        "classe2 = 0\n",
        "for i in range(len(dataset)):\n",
        "  if dataset[i][2]==5:\n",
        "    labels[i]=1\n",
        "    classe1 += 1\n",
        "  else:\n",
        "    labels[i]=0\n",
        "    classe2 += 1\n",
        "print(labels.shape)\n",
        "\n",
        "print(classe1, classe2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(342500,)\n",
            "171250 171250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCIY9FU_PeFg"
      },
      "source": [
        "Eliminiamo la terza colonna del dataset, che contiene i flavor (usati per le labels)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hStRrP0VQJx",
        "outputId": "824b9586-09af-4632-c09f-72558f0c5601"
      },
      "source": [
        "data = np.delete(dataset, 2, 1)\n",
        "print(data.shape)\n",
        "print(data)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(342500, 4)\n",
            "[[24.8372 2.14779\n",
            "  list([40.0829, 38.9911, 42.8764, 17.6421, 4, 3.44542e-17, 0.144208, 0.252595])\n",
            "  list([76.4005, 1, 2, 0.38647299999999996, 0.319795, 0.18357199999999999])]\n",
            " [24.4613 1.8719800000000002\n",
            "  list([0.9655239999999999, '-inf', 0.542345, '-inf', 0, 0.10756099999999999, 0.0294951, 0.0981485])\n",
            "  list([-1, -1, -1, 'inf', -1, '-inf'])]\n",
            " [28.2742 -1.06965\n",
            "  list([21.8931, 13.5264, 3.95583, 2.00677, 6, 1.0714600000000003e-15, 0.095354, 0.138671])\n",
            "  list([88.4217, 2, 5, 0.113212, 1.98228, 0.6210519999999999])]\n",
            " ...\n",
            " [25.7177 0.421292\n",
            "  list([4.02657, 0.136679, 0.0224637, 0.47055, 2, 0.0007335340000000001, 0.165536, 0.10527])\n",
            "  list([-1, -1, -1, 'inf', -1, '-inf'])]\n",
            " [33.7001 -0.44517799999999996\n",
            "  list([1.50458, '-inf', 1.53197, '-inf', 1, 0.000434408, 0.106206, 0.0702319])\n",
            "  list([35.2886, 1, 2, 0.11746000000000001, 1.42181, 0.973598])]\n",
            " [26.0728 -2.4862\n",
            "  list(['-inf', '-inf', '-inf', '-inf', 0, 0.057028699999999995, 0.177813, 0.143865])\n",
            "  list([-1, -1, -1, 'inf', -1, '-inf'])]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl5W-3gDWGlV"
      },
      "source": [
        "Scriviamo il nuovo dataframe estrapolando le variabili di tracking e vertex."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWdKc9y-E_-G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f20aee6-0ca8-464e-b4d3-84ac0ffa33eb"
      },
      "source": [
        "nrighe = len((data))\n",
        "print(nrighe)\n",
        "\n",
        "tracking_variables = np.zeros((nrighe,8))\n",
        "for j in range(8):\n",
        "  for i in range(nrighe):\n",
        "    tracking_variables[i][j] = data[i][2][j]\n",
        "print(tracking_variables.shape)\n",
        "\n",
        "vertex_variables = np.zeros((nrighe,6))\n",
        "for j in range(6):\n",
        "  for i in range(nrighe):\n",
        "    vertex_variables[i][j] = data[i][3][j]\n",
        "print(vertex_variables.shape)\n",
        "\n",
        "prime_due_colonne= np.zeros((nrighe,2))\n",
        "for j in range(2):\n",
        "  for i in range(nrighe):\n",
        "    prime_due_colonne[i][j]=data[i][j]\n",
        "\n",
        "high_level_variables = np.concatenate((tracking_variables, vertex_variables), axis=1)\n",
        "print(high_level_variables.shape)\n",
        "data_completo=np.concatenate((prime_due_colonne, high_level_variables), axis=1)\n",
        "print(data_completo.shape)\n",
        "\n",
        "\n",
        "df= pd.DataFrame(data_completo)\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "342500\n",
            "(342500, 8)\n",
            "(342500, 6)\n",
            "(342500, 14)\n",
            "(342500, 16)\n",
            "             0         1          2          3          4          5    6   \\\n",
            "0       24.8372  2.147790  40.082900  38.991100  42.876400  17.642100  4.0   \n",
            "1       24.4613  1.871980   0.965524       -inf   0.542345       -inf  0.0   \n",
            "2       28.2742 -1.069650  21.893100  13.526400   3.955830   2.006770  6.0   \n",
            "3       39.1396  1.652650  10.191400  16.124900   1.771140  14.301600  5.0   \n",
            "4       37.1912  0.540546   3.581540   3.022570   1.593930   0.028418  5.0   \n",
            "...         ...       ...        ...        ...        ...        ...  ...   \n",
            "342495  36.6842 -0.094224   1.282910   0.588207   0.862186   0.660734  1.0   \n",
            "342496  36.2943 -1.144770   9.067270       -inf   1.607130       -inf  2.0   \n",
            "342497  25.7177  0.421292   4.026570   0.136679   0.022464   0.470550  2.0   \n",
            "342498  33.7001 -0.445178   1.504580       -inf   1.531970       -inf  1.0   \n",
            "342499  26.0728 -2.486200       -inf       -inf       -inf       -inf  0.0   \n",
            "\n",
            "                  7         8         9         10   11   12        13  \\\n",
            "0       3.445420e-17  0.144208  0.252595   76.4005  1.0  2.0  0.386473   \n",
            "1       1.075610e-01  0.029495  0.098148   -1.0000 -1.0 -1.0       inf   \n",
            "2       1.071460e-15  0.095354  0.138671   88.4217  2.0  5.0  0.113212   \n",
            "3       2.105910e-10  0.069339  0.104544  168.3500  2.0  7.0  0.103774   \n",
            "4       5.445090e-07  0.087980  0.055941   72.0860  2.0  7.0  0.087989   \n",
            "...              ...       ...       ...       ...  ...  ...       ...   \n",
            "342495  3.768520e-03  0.083181  0.118200   -1.0000 -1.0 -1.0       inf   \n",
            "342496  7.473350e-07  0.028702  0.260847   13.5455  1.0  2.0  0.123076   \n",
            "342497  7.335340e-04  0.165536  0.105270   -1.0000 -1.0 -1.0       inf   \n",
            "342498  4.344080e-04  0.106206  0.070232   35.2886  1.0  2.0  0.117460   \n",
            "342499  5.702870e-02  0.177813  0.143865   -1.0000 -1.0 -1.0       inf   \n",
            "\n",
            "              14        15  \n",
            "0       0.319795  0.183572  \n",
            "1      -1.000000      -inf  \n",
            "2       1.982280  0.621052  \n",
            "3       2.910530  1.155780  \n",
            "4       3.129650  0.711967  \n",
            "...          ...       ...  \n",
            "342495 -1.000000      -inf  \n",
            "342496  0.533815  0.999963  \n",
            "342497 -1.000000      -inf  \n",
            "342498  1.421810  0.973598  \n",
            "342499 -1.000000      -inf  \n",
            "\n",
            "[342500 rows x 16 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPHolAnbQVug"
      },
      "source": [
        "Normalizzazione dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RsmNFaFipPA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 882
        },
        "outputId": "3e04fedd-4362-4e56-ccc6-53b3b8d613a2"
      },
      "source": [
        "#sostituiamo i +inf e -inf con dei NaN, che vengono ignorati nel calcolo della media.\n",
        "df = df.applymap(lambda x: np.nan if x == np.inf  else x)\n",
        "df = df.applymap(lambda x: np.nan if x == np.NINF  else x)\n",
        "\n",
        "train_to_rest_ratio=0.8\n",
        "vali_to_test_ratio=0.5\n",
        "\n",
        "#calcoliamo i parametri per la normalizzazione solo con il training set\n",
        "n_time_steps=int(len(data)*train_to_rest_ratio)\n",
        "data_mean = df.iloc[:n_time_steps].mean()\n",
        "data_std = df.iloc[:n_time_steps].std()\n",
        "\n",
        "\n",
        "df = (df - data_mean)/data_std\n",
        "df = pd.DataFrame(df)\n",
        "#sostituiamo i NaN con 0.\n",
        "df =df.fillna(0)\n",
        "\n",
        "#split dei dati\n",
        "n_time_steps_2 = int(len(data)*(1-train_to_rest_ratio)*vali_to_test_ratio)\n",
        "train_data = df.iloc[:n_time_steps]\n",
        "test_data = df.iloc[(n_time_steps):]\n",
        "\n",
        "\n",
        "train_labels = labels[:n_time_steps]\n",
        "test_labels = labels[n_time_steps:]\n",
        "\n",
        "display(train_data)\n",
        "display(test_data)\n",
        "\n",
        "X_train=np.array(train_data)\n",
        "X_test=np.array(test_data)\n",
        "\n",
        "print(train_labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "              0         1         2         3         4         5         6   \\\n",
              "0      -0.678880  1.551626  4.532298  6.870447  2.068997  1.301436  1.461494   \n",
              "1      -0.710477  1.352423 -0.472327  0.000000 -0.197927  0.000000 -0.885043   \n",
              "2      -0.389977 -0.772161  2.205119  2.021459 -0.015140 -0.080786  2.634763   \n",
              "3       0.523331  1.194012  0.708019  2.516266 -0.132127  1.006123  2.048128   \n",
              "4       0.359555  0.390799 -0.137637  0.021320 -0.141617 -0.255680  2.048128   \n",
              "...          ...       ...       ...       ...       ...       ...       ...   \n",
              "273995  0.057742  0.564079 -0.506472 -0.530670 -0.132413 -0.246187 -0.298408   \n",
              "273996 -0.725103 -0.190378 -0.529629  0.000000 -0.213295  0.000000 -0.298408   \n",
              "273997  1.219101 -0.514269 -0.225485 -0.176083 -0.211333 -0.226565  0.874860   \n",
              "273998  1.649841  1.036541 -0.431090 -0.335674 -0.138261 -0.169800 -0.298408   \n",
              "273999 -0.685546 -0.041429 -0.543722  0.000000 -0.164135  0.000000 -0.885043   \n",
              "\n",
              "              7         8         9         10        11        12        13  \\\n",
              "0      -0.241201  0.911836  2.704066  0.317607  0.715920  0.208219 -0.069210   \n",
              "1       0.704568 -1.455366 -0.223216 -0.456157 -0.809132 -0.677520  0.000000   \n",
              "2      -0.241201 -0.096309  0.544822  0.437781  1.478446  1.093957 -0.347008   \n",
              "3      -0.241201 -0.633144 -0.101999  1.236815  1.478446  1.684450 -0.356602   \n",
              "4      -0.241197 -0.248474 -1.023190  0.274475  1.478446  1.684450 -0.372649   \n",
              "...          ...       ...       ...       ...       ...       ...       ...   \n",
              "273995 -0.146219 -0.066744 -0.426762 -0.456157 -0.809132 -0.677520  0.000000   \n",
              "273996 -0.210692 -0.744718  2.230288 -0.456157 -0.809132 -0.677520  0.000000   \n",
              "273997 -0.241173 -0.168781  1.055767 -0.456157 -0.809132 -0.677520  0.000000   \n",
              "273998 -0.207085 -0.331410  0.084938 -0.456157 -0.809132 -0.677520  0.000000   \n",
              "273999  0.115311 -1.658128 -0.308400 -0.456157 -0.809132 -0.677520  0.000000   \n",
              "\n",
              "              14        15  \n",
              "0       0.075666 -1.352884  \n",
              "1      -0.713615  0.000000  \n",
              "2       1.069886 -0.352136  \n",
              "3       1.625010  0.871069  \n",
              "4       1.756051 -0.144165  \n",
              "...          ...       ...  \n",
              "273995 -0.713615  0.000000  \n",
              "273996 -0.713615  0.000000  \n",
              "273997 -0.713615  0.000000  \n",
              "273998 -0.713615  0.000000  \n",
              "273999 -0.713615  0.000000  \n",
              "\n",
              "[274000 rows x 16 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-874a7a9a-7710-4037-99b4-f020b1d4052c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.678880</td>\n",
              "      <td>1.551626</td>\n",
              "      <td>4.532298</td>\n",
              "      <td>6.870447</td>\n",
              "      <td>2.068997</td>\n",
              "      <td>1.301436</td>\n",
              "      <td>1.461494</td>\n",
              "      <td>-0.241201</td>\n",
              "      <td>0.911836</td>\n",
              "      <td>2.704066</td>\n",
              "      <td>0.317607</td>\n",
              "      <td>0.715920</td>\n",
              "      <td>0.208219</td>\n",
              "      <td>-0.069210</td>\n",
              "      <td>0.075666</td>\n",
              "      <td>-1.352884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.710477</td>\n",
              "      <td>1.352423</td>\n",
              "      <td>-0.472327</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.197927</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.885043</td>\n",
              "      <td>0.704568</td>\n",
              "      <td>-1.455366</td>\n",
              "      <td>-0.223216</td>\n",
              "      <td>-0.456157</td>\n",
              "      <td>-0.809132</td>\n",
              "      <td>-0.677520</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.713615</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.389977</td>\n",
              "      <td>-0.772161</td>\n",
              "      <td>2.205119</td>\n",
              "      <td>2.021459</td>\n",
              "      <td>-0.015140</td>\n",
              "      <td>-0.080786</td>\n",
              "      <td>2.634763</td>\n",
              "      <td>-0.241201</td>\n",
              "      <td>-0.096309</td>\n",
              "      <td>0.544822</td>\n",
              "      <td>0.437781</td>\n",
              "      <td>1.478446</td>\n",
              "      <td>1.093957</td>\n",
              "      <td>-0.347008</td>\n",
              "      <td>1.069886</td>\n",
              "      <td>-0.352136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.523331</td>\n",
              "      <td>1.194012</td>\n",
              "      <td>0.708019</td>\n",
              "      <td>2.516266</td>\n",
              "      <td>-0.132127</td>\n",
              "      <td>1.006123</td>\n",
              "      <td>2.048128</td>\n",
              "      <td>-0.241201</td>\n",
              "      <td>-0.633144</td>\n",
              "      <td>-0.101999</td>\n",
              "      <td>1.236815</td>\n",
              "      <td>1.478446</td>\n",
              "      <td>1.684450</td>\n",
              "      <td>-0.356602</td>\n",
              "      <td>1.625010</td>\n",
              "      <td>0.871069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.359555</td>\n",
              "      <td>0.390799</td>\n",
              "      <td>-0.137637</td>\n",
              "      <td>0.021320</td>\n",
              "      <td>-0.141617</td>\n",
              "      <td>-0.255680</td>\n",
              "      <td>2.048128</td>\n",
              "      <td>-0.241197</td>\n",
              "      <td>-0.248474</td>\n",
              "      <td>-1.023190</td>\n",
              "      <td>0.274475</td>\n",
              "      <td>1.478446</td>\n",
              "      <td>1.684450</td>\n",
              "      <td>-0.372649</td>\n",
              "      <td>1.756051</td>\n",
              "      <td>-0.144165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273995</th>\n",
              "      <td>0.057742</td>\n",
              "      <td>0.564079</td>\n",
              "      <td>-0.506472</td>\n",
              "      <td>-0.530670</td>\n",
              "      <td>-0.132413</td>\n",
              "      <td>-0.246187</td>\n",
              "      <td>-0.298408</td>\n",
              "      <td>-0.146219</td>\n",
              "      <td>-0.066744</td>\n",
              "      <td>-0.426762</td>\n",
              "      <td>-0.456157</td>\n",
              "      <td>-0.809132</td>\n",
              "      <td>-0.677520</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.713615</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273996</th>\n",
              "      <td>-0.725103</td>\n",
              "      <td>-0.190378</td>\n",
              "      <td>-0.529629</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.213295</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.298408</td>\n",
              "      <td>-0.210692</td>\n",
              "      <td>-0.744718</td>\n",
              "      <td>2.230288</td>\n",
              "      <td>-0.456157</td>\n",
              "      <td>-0.809132</td>\n",
              "      <td>-0.677520</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.713615</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273997</th>\n",
              "      <td>1.219101</td>\n",
              "      <td>-0.514269</td>\n",
              "      <td>-0.225485</td>\n",
              "      <td>-0.176083</td>\n",
              "      <td>-0.211333</td>\n",
              "      <td>-0.226565</td>\n",
              "      <td>0.874860</td>\n",
              "      <td>-0.241173</td>\n",
              "      <td>-0.168781</td>\n",
              "      <td>1.055767</td>\n",
              "      <td>-0.456157</td>\n",
              "      <td>-0.809132</td>\n",
              "      <td>-0.677520</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.713615</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273998</th>\n",
              "      <td>1.649841</td>\n",
              "      <td>1.036541</td>\n",
              "      <td>-0.431090</td>\n",
              "      <td>-0.335674</td>\n",
              "      <td>-0.138261</td>\n",
              "      <td>-0.169800</td>\n",
              "      <td>-0.298408</td>\n",
              "      <td>-0.207085</td>\n",
              "      <td>-0.331410</td>\n",
              "      <td>0.084938</td>\n",
              "      <td>-0.456157</td>\n",
              "      <td>-0.809132</td>\n",
              "      <td>-0.677520</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.713615</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273999</th>\n",
              "      <td>-0.685546</td>\n",
              "      <td>-0.041429</td>\n",
              "      <td>-0.543722</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.164135</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.885043</td>\n",
              "      <td>0.115311</td>\n",
              "      <td>-1.658128</td>\n",
              "      <td>-0.308400</td>\n",
              "      <td>-0.456157</td>\n",
              "      <td>-0.809132</td>\n",
              "      <td>-0.677520</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.713615</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>274000 rows × 16 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-874a7a9a-7710-4037-99b4-f020b1d4052c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-874a7a9a-7710-4037-99b4-f020b1d4052c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-874a7a9a-7710-4037-99b4-f020b1d4052c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "              0         1         2         3         4         5         6   \\\n",
              "274000  0.031054  0.398260 -0.533686  0.000000 -0.164028  0.000000 -0.298408   \n",
              "274001 -0.424903  0.520999  0.004029  0.288963 -0.138903 -0.112782  2.048128   \n",
              "274002 -0.689034 -0.101573 -0.561707  0.000000 -0.204871  0.000000 -0.298408   \n",
              "274003 -0.416413 -0.078225 -0.491004 -0.430832 -0.173014 -0.217986 -0.298408   \n",
              "274004 -0.514364  0.997128 -0.555863 -0.515564 -0.170809 -0.123824 -0.298408   \n",
              "...          ...       ...       ...       ...       ...       ...       ...   \n",
              "342495  0.316939 -0.067662 -0.431721 -0.442231 -0.180800 -0.199781 -0.298408   \n",
              "342496  0.284165 -0.826416  0.564200  0.000000 -0.140910  0.000000  0.288226   \n",
              "342497 -0.604868  0.304668 -0.080701 -0.528211 -0.225766 -0.216594  0.288226   \n",
              "342498  0.066105 -0.321138 -0.403361  0.000000 -0.144934  0.000000 -0.298408   \n",
              "342499 -0.575020 -1.795259  0.000000  0.000000  0.000000  0.000000 -0.885043   \n",
              "\n",
              "              7         8         9         10        11        12        13  \\\n",
              "274000 -0.211806  2.285484 -0.681381 -0.456157 -0.809132 -0.677520  0.000000   \n",
              "274001 -0.241201 -0.282026 -0.009734  0.259305  1.478446  1.979696 -0.370612   \n",
              "274002 -0.224833 -0.611561  0.451970 -0.456157 -0.809132 -0.677520  0.000000   \n",
              "274003 -0.129419  2.317676  0.703974 -0.456157 -0.809132 -0.677520  0.000000   \n",
              "274004  0.155100 -1.776560 -0.018889 -0.456157 -0.809132 -0.677520  0.000000   \n",
              "...          ...       ...       ...       ...       ...       ...       ...   \n",
              "342495 -0.208065 -0.347518  0.156828 -0.456157 -0.809132 -0.677520  0.000000   \n",
              "342496 -0.241195 -1.471741  2.860469 -0.310747  0.715920  0.208219 -0.336980   \n",
              "342497 -0.234751  1.351958 -0.088239 -0.456157 -0.809132 -0.677520  0.000000   \n",
              "342498 -0.237382  0.127631 -0.752329 -0.093384  0.715920  0.208219 -0.342689   \n",
              "342499  0.260245  1.605305  0.643266 -0.456157 -0.809132 -0.677520  0.000000   \n",
              "\n",
              "              14        15  \n",
              "274000 -0.713615  0.000000  \n",
              "274001  2.699496  2.131932  \n",
              "274002 -0.713615  0.000000  \n",
              "274003 -0.713615  0.000000  \n",
              "274004 -0.713615  0.000000  \n",
              "...          ...       ...  \n",
              "342495 -0.713615  0.000000  \n",
              "342496  0.203657  0.514634  \n",
              "342497 -0.713615  0.000000  \n",
              "342498  0.734707  0.454323  \n",
              "342499 -0.713615  0.000000  \n",
              "\n",
              "[68500 rows x 16 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8081fcb5-88fa-47a1-9e3a-a5ba963fab75\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>274000</th>\n",
              "      <td>0.031054</td>\n",
              "      <td>0.398260</td>\n",
              "      <td>-0.533686</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.164028</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.298408</td>\n",
              "      <td>-0.211806</td>\n",
              "      <td>2.285484</td>\n",
              "      <td>-0.681381</td>\n",
              "      <td>-0.456157</td>\n",
              "      <td>-0.809132</td>\n",
              "      <td>-0.677520</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.713615</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274001</th>\n",
              "      <td>-0.424903</td>\n",
              "      <td>0.520999</td>\n",
              "      <td>0.004029</td>\n",
              "      <td>0.288963</td>\n",
              "      <td>-0.138903</td>\n",
              "      <td>-0.112782</td>\n",
              "      <td>2.048128</td>\n",
              "      <td>-0.241201</td>\n",
              "      <td>-0.282026</td>\n",
              "      <td>-0.009734</td>\n",
              "      <td>0.259305</td>\n",
              "      <td>1.478446</td>\n",
              "      <td>1.979696</td>\n",
              "      <td>-0.370612</td>\n",
              "      <td>2.699496</td>\n",
              "      <td>2.131932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274002</th>\n",
              "      <td>-0.689034</td>\n",
              "      <td>-0.101573</td>\n",
              "      <td>-0.561707</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.204871</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.298408</td>\n",
              "      <td>-0.224833</td>\n",
              "      <td>-0.611561</td>\n",
              "      <td>0.451970</td>\n",
              "      <td>-0.456157</td>\n",
              "      <td>-0.809132</td>\n",
              "      <td>-0.677520</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.713615</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274003</th>\n",
              "      <td>-0.416413</td>\n",
              "      <td>-0.078225</td>\n",
              "      <td>-0.491004</td>\n",
              "      <td>-0.430832</td>\n",
              "      <td>-0.173014</td>\n",
              "      <td>-0.217986</td>\n",
              "      <td>-0.298408</td>\n",
              "      <td>-0.129419</td>\n",
              "      <td>2.317676</td>\n",
              "      <td>0.703974</td>\n",
              "      <td>-0.456157</td>\n",
              "      <td>-0.809132</td>\n",
              "      <td>-0.677520</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.713615</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274004</th>\n",
              "      <td>-0.514364</td>\n",
              "      <td>0.997128</td>\n",
              "      <td>-0.555863</td>\n",
              "      <td>-0.515564</td>\n",
              "      <td>-0.170809</td>\n",
              "      <td>-0.123824</td>\n",
              "      <td>-0.298408</td>\n",
              "      <td>0.155100</td>\n",
              "      <td>-1.776560</td>\n",
              "      <td>-0.018889</td>\n",
              "      <td>-0.456157</td>\n",
              "      <td>-0.809132</td>\n",
              "      <td>-0.677520</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.713615</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342495</th>\n",
              "      <td>0.316939</td>\n",
              "      <td>-0.067662</td>\n",
              "      <td>-0.431721</td>\n",
              "      <td>-0.442231</td>\n",
              "      <td>-0.180800</td>\n",
              "      <td>-0.199781</td>\n",
              "      <td>-0.298408</td>\n",
              "      <td>-0.208065</td>\n",
              "      <td>-0.347518</td>\n",
              "      <td>0.156828</td>\n",
              "      <td>-0.456157</td>\n",
              "      <td>-0.809132</td>\n",
              "      <td>-0.677520</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.713615</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342496</th>\n",
              "      <td>0.284165</td>\n",
              "      <td>-0.826416</td>\n",
              "      <td>0.564200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.140910</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.288226</td>\n",
              "      <td>-0.241195</td>\n",
              "      <td>-1.471741</td>\n",
              "      <td>2.860469</td>\n",
              "      <td>-0.310747</td>\n",
              "      <td>0.715920</td>\n",
              "      <td>0.208219</td>\n",
              "      <td>-0.336980</td>\n",
              "      <td>0.203657</td>\n",
              "      <td>0.514634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342497</th>\n",
              "      <td>-0.604868</td>\n",
              "      <td>0.304668</td>\n",
              "      <td>-0.080701</td>\n",
              "      <td>-0.528211</td>\n",
              "      <td>-0.225766</td>\n",
              "      <td>-0.216594</td>\n",
              "      <td>0.288226</td>\n",
              "      <td>-0.234751</td>\n",
              "      <td>1.351958</td>\n",
              "      <td>-0.088239</td>\n",
              "      <td>-0.456157</td>\n",
              "      <td>-0.809132</td>\n",
              "      <td>-0.677520</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.713615</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342498</th>\n",
              "      <td>0.066105</td>\n",
              "      <td>-0.321138</td>\n",
              "      <td>-0.403361</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.144934</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.298408</td>\n",
              "      <td>-0.237382</td>\n",
              "      <td>0.127631</td>\n",
              "      <td>-0.752329</td>\n",
              "      <td>-0.093384</td>\n",
              "      <td>0.715920</td>\n",
              "      <td>0.208219</td>\n",
              "      <td>-0.342689</td>\n",
              "      <td>0.734707</td>\n",
              "      <td>0.454323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342499</th>\n",
              "      <td>-0.575020</td>\n",
              "      <td>-1.795259</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.885043</td>\n",
              "      <td>0.260245</td>\n",
              "      <td>1.605305</td>\n",
              "      <td>0.643266</td>\n",
              "      <td>-0.456157</td>\n",
              "      <td>-0.809132</td>\n",
              "      <td>-0.677520</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.713615</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>68500 rows × 16 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8081fcb5-88fa-47a1-9e3a-a5ba963fab75')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8081fcb5-88fa-47a1-9e3a-a5ba963fab75 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8081fcb5-88fa-47a1-9e3a-a5ba963fab75');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(274000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LiMore-NVsi",
        "outputId": "6fced362-64b2-4dfb-8ac0-0b815612e64f"
      },
      "source": [
        "#applichiamo alle labels il one_hot encoding\n",
        "# N.B. runnare solo 1 volta\n",
        "\n",
        "train_labels = keras.utils.to_categorical(train_labels)\n",
        "test_labels = keras.utils.to_categorical(test_labels)\n",
        "\n",
        "print(train_labels.shape)\n",
        "print(train_labels[1000])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(274000, 2)\n",
            "[1. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkIyLpeEPoIs"
      },
      "source": [
        "#definiamo il modello\n",
        "keras.backend.clear_session()\n",
        "model = keras.Sequential([\n",
        "                          keras.layers.Dense(64, activation='relu'),\n",
        "                          keras.layers.Dense(32, activation='relu'),\n",
        "                          keras.layers.Dense(16, activation='relu'),\n",
        "                          keras.layers.Dense(8, activation='relu'),\n",
        "                          keras.layers.Dense(2, activation='softmax'),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tQAUAxyRhzq"
      },
      "source": [
        "#compiliamo il modello\n",
        "\n",
        "model.compile(optimizer='sgd',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['AUC'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(train_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm-ovGimB4Br",
        "outputId": "d31a4ace-77f1-46a1-d53a-1331291e21b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(274000, 16)\n",
            "(274000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsOwC_sfSS0z",
        "outputId": "39e04b10-b261-4a68-a213-cebe704b1a41"
      },
      "source": [
        "#applichiamo il modello ai dati\n",
        "\n",
        "history = model.fit(X_train, train_labels, epochs=100, batch_size=100,\n",
        "                    validation_split=0.2, shuffle=True, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2192/2192 [==============================] - 8s 3ms/step - loss: 0.4133 - auc: 0.8948 - val_loss: 0.3647 - val_auc: 0.9163\n",
            "Epoch 2/100\n",
            "2192/2192 [==============================] - 3s 1ms/step - loss: 0.3620 - auc: 0.9171 - val_loss: 0.3605 - val_auc: 0.9180\n",
            "Epoch 3/100\n",
            "2192/2192 [==============================] - 3s 1ms/step - loss: 0.3589 - auc: 0.9184 - val_loss: 0.3582 - val_auc: 0.9191\n",
            "Epoch 4/100\n",
            "2192/2192 [==============================] - 3s 2ms/step - loss: 0.3568 - auc: 0.9194 - val_loss: 0.3568 - val_auc: 0.9196\n",
            "Epoch 5/100\n",
            "2192/2192 [==============================] - 3s 2ms/step - loss: 0.3552 - auc: 0.9201 - val_loss: 0.3552 - val_auc: 0.9203\n",
            "Epoch 6/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3538 - auc: 0.9207 - val_loss: 0.3542 - val_auc: 0.9206\n",
            "Epoch 7/100\n",
            "2192/2192 [==============================] - 3s 2ms/step - loss: 0.3525 - auc: 0.9212 - val_loss: 0.3528 - val_auc: 0.9212\n",
            "Epoch 8/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3514 - auc: 0.9216 - val_loss: 0.3519 - val_auc: 0.9218\n",
            "Epoch 9/100\n",
            "2192/2192 [==============================] - 3s 2ms/step - loss: 0.3503 - auc: 0.9222 - val_loss: 0.3516 - val_auc: 0.9218\n",
            "Epoch 10/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3494 - auc: 0.9225 - val_loss: 0.3498 - val_auc: 0.9226\n",
            "Epoch 11/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3484 - auc: 0.9230 - val_loss: 0.3489 - val_auc: 0.9229\n",
            "Epoch 12/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3475 - auc: 0.9233 - val_loss: 0.3483 - val_auc: 0.9231\n",
            "Epoch 13/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3466 - auc: 0.9237 - val_loss: 0.3475 - val_auc: 0.9235\n",
            "Epoch 14/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3458 - auc: 0.9240 - val_loss: 0.3465 - val_auc: 0.9238\n",
            "Epoch 15/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3450 - auc: 0.9244 - val_loss: 0.3460 - val_auc: 0.9243\n",
            "Epoch 16/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3443 - auc: 0.9247 - val_loss: 0.3460 - val_auc: 0.9240\n",
            "Epoch 17/100\n",
            "2192/2192 [==============================] - 3s 2ms/step - loss: 0.3436 - auc: 0.9250 - val_loss: 0.3449 - val_auc: 0.9247\n",
            "Epoch 18/100\n",
            "2192/2192 [==============================] - 3s 2ms/step - loss: 0.3432 - auc: 0.9252 - val_loss: 0.3448 - val_auc: 0.9249\n",
            "Epoch 19/100\n",
            "2192/2192 [==============================] - 3s 2ms/step - loss: 0.3426 - auc: 0.9254 - val_loss: 0.3442 - val_auc: 0.9252\n",
            "Epoch 20/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3421 - auc: 0.9256 - val_loss: 0.3433 - val_auc: 0.9254\n",
            "Epoch 21/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3417 - auc: 0.9258 - val_loss: 0.3435 - val_auc: 0.9256\n",
            "Epoch 22/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3413 - auc: 0.9260 - val_loss: 0.3433 - val_auc: 0.9249\n",
            "Epoch 23/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3409 - auc: 0.9261 - val_loss: 0.3432 - val_auc: 0.9252\n",
            "Epoch 24/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3404 - auc: 0.9263 - val_loss: 0.3432 - val_auc: 0.9258\n",
            "Epoch 25/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3402 - auc: 0.9264 - val_loss: 0.3423 - val_auc: 0.9261\n",
            "Epoch 26/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3397 - auc: 0.9266 - val_loss: 0.3422 - val_auc: 0.9261\n",
            "Epoch 27/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3395 - auc: 0.9267 - val_loss: 0.3432 - val_auc: 0.9255\n",
            "Epoch 28/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3391 - auc: 0.9269 - val_loss: 0.3412 - val_auc: 0.9263\n",
            "Epoch 29/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3388 - auc: 0.9270 - val_loss: 0.3407 - val_auc: 0.9264\n",
            "Epoch 30/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3385 - auc: 0.9271 - val_loss: 0.3423 - val_auc: 0.9255\n",
            "Epoch 31/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3382 - auc: 0.9272 - val_loss: 0.3407 - val_auc: 0.9265\n",
            "Epoch 32/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3380 - auc: 0.9273 - val_loss: 0.3407 - val_auc: 0.9265\n",
            "Epoch 33/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3377 - auc: 0.9275 - val_loss: 0.3406 - val_auc: 0.9258\n",
            "Epoch 34/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3374 - auc: 0.9275 - val_loss: 0.3400 - val_auc: 0.9265\n",
            "Epoch 35/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3372 - auc: 0.9276 - val_loss: 0.3398 - val_auc: 0.9273\n",
            "Epoch 36/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3370 - auc: 0.9277 - val_loss: 0.3391 - val_auc: 0.9271\n",
            "Epoch 37/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3367 - auc: 0.9279 - val_loss: 0.3386 - val_auc: 0.9273\n",
            "Epoch 38/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3366 - auc: 0.9279 - val_loss: 0.3382 - val_auc: 0.9274\n",
            "Epoch 39/100\n",
            "2192/2192 [==============================] - 5s 2ms/step - loss: 0.3363 - auc: 0.9280 - val_loss: 0.3380 - val_auc: 0.9273\n",
            "Epoch 40/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3360 - auc: 0.9281 - val_loss: 0.3389 - val_auc: 0.9274\n",
            "Epoch 41/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3358 - auc: 0.9281 - val_loss: 0.3381 - val_auc: 0.9277\n",
            "Epoch 42/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3356 - auc: 0.9282 - val_loss: 0.3394 - val_auc: 0.9264\n",
            "Epoch 43/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3353 - auc: 0.9284 - val_loss: 0.3379 - val_auc: 0.9274\n",
            "Epoch 44/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3350 - auc: 0.9284 - val_loss: 0.3372 - val_auc: 0.9280\n",
            "Epoch 45/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3348 - auc: 0.9286 - val_loss: 0.3379 - val_auc: 0.9277\n",
            "Epoch 46/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3346 - auc: 0.9287 - val_loss: 0.3385 - val_auc: 0.9267\n",
            "Epoch 47/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3345 - auc: 0.9287 - val_loss: 0.3370 - val_auc: 0.9276\n",
            "Epoch 48/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3343 - auc: 0.9287 - val_loss: 0.3375 - val_auc: 0.9278\n",
            "Epoch 49/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3341 - auc: 0.9289 - val_loss: 0.3368 - val_auc: 0.9282\n",
            "Epoch 50/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3337 - auc: 0.9290 - val_loss: 0.3368 - val_auc: 0.9280\n",
            "Epoch 51/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3335 - auc: 0.9291 - val_loss: 0.3364 - val_auc: 0.9279\n",
            "Epoch 52/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3333 - auc: 0.9293 - val_loss: 0.3372 - val_auc: 0.9277\n",
            "Epoch 53/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3331 - auc: 0.9293 - val_loss: 0.3368 - val_auc: 0.9285\n",
            "Epoch 54/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3329 - auc: 0.9293 - val_loss: 0.3361 - val_auc: 0.9282\n",
            "Epoch 55/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3327 - auc: 0.9294 - val_loss: 0.3382 - val_auc: 0.9278\n",
            "Epoch 56/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3326 - auc: 0.9295 - val_loss: 0.3352 - val_auc: 0.9285\n",
            "Epoch 57/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3324 - auc: 0.9295 - val_loss: 0.3357 - val_auc: 0.9284\n",
            "Epoch 58/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3322 - auc: 0.9296 - val_loss: 0.3359 - val_auc: 0.9289\n",
            "Epoch 59/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3319 - auc: 0.9298 - val_loss: 0.3385 - val_auc: 0.9267\n",
            "Epoch 60/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3317 - auc: 0.9299 - val_loss: 0.3350 - val_auc: 0.9289\n",
            "Epoch 61/100\n",
            "2192/2192 [==============================] - 3s 2ms/step - loss: 0.3315 - auc: 0.9299 - val_loss: 0.3354 - val_auc: 0.9287\n",
            "Epoch 62/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3313 - auc: 0.9300 - val_loss: 0.3355 - val_auc: 0.9284\n",
            "Epoch 63/100\n",
            "2192/2192 [==============================] - 3s 2ms/step - loss: 0.3313 - auc: 0.9300 - val_loss: 0.3356 - val_auc: 0.9285\n",
            "Epoch 64/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3310 - auc: 0.9301 - val_loss: 0.3354 - val_auc: 0.9287\n",
            "Epoch 65/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3308 - auc: 0.9302 - val_loss: 0.3341 - val_auc: 0.9289\n",
            "Epoch 66/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3305 - auc: 0.9303 - val_loss: 0.3335 - val_auc: 0.9292\n",
            "Epoch 67/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3305 - auc: 0.9304 - val_loss: 0.3349 - val_auc: 0.9286\n",
            "Epoch 68/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3302 - auc: 0.9305 - val_loss: 0.3343 - val_auc: 0.9284\n",
            "Epoch 69/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3300 - auc: 0.9305 - val_loss: 0.3339 - val_auc: 0.9290\n",
            "Epoch 70/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3298 - auc: 0.9306 - val_loss: 0.3342 - val_auc: 0.9294\n",
            "Epoch 71/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3297 - auc: 0.9306 - val_loss: 0.3335 - val_auc: 0.9297\n",
            "Epoch 72/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3295 - auc: 0.9307 - val_loss: 0.3366 - val_auc: 0.9284\n",
            "Epoch 73/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3294 - auc: 0.9307 - val_loss: 0.3344 - val_auc: 0.9288\n",
            "Epoch 74/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3293 - auc: 0.9308 - val_loss: 0.3347 - val_auc: 0.9286\n",
            "Epoch 75/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3291 - auc: 0.9308 - val_loss: 0.3328 - val_auc: 0.9296\n",
            "Epoch 76/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3290 - auc: 0.9309 - val_loss: 0.3334 - val_auc: 0.9298\n",
            "Epoch 77/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3287 - auc: 0.9310 - val_loss: 0.3331 - val_auc: 0.9295\n",
            "Epoch 78/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3286 - auc: 0.9310 - val_loss: 0.3323 - val_auc: 0.9298\n",
            "Epoch 79/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3286 - auc: 0.9311 - val_loss: 0.3323 - val_auc: 0.9297\n",
            "Epoch 80/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3283 - auc: 0.9312 - val_loss: 0.3322 - val_auc: 0.9296\n",
            "Epoch 81/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3281 - auc: 0.9312 - val_loss: 0.3323 - val_auc: 0.9296\n",
            "Epoch 82/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3281 - auc: 0.9312 - val_loss: 0.3319 - val_auc: 0.9301\n",
            "Epoch 83/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3278 - auc: 0.9314 - val_loss: 0.3318 - val_auc: 0.9300\n",
            "Epoch 84/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3278 - auc: 0.9314 - val_loss: 0.3327 - val_auc: 0.9292\n",
            "Epoch 85/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3277 - auc: 0.9314 - val_loss: 0.3317 - val_auc: 0.9297\n",
            "Epoch 86/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3276 - auc: 0.9314 - val_loss: 0.3316 - val_auc: 0.9301\n",
            "Epoch 87/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3273 - auc: 0.9316 - val_loss: 0.3314 - val_auc: 0.9304\n",
            "Epoch 88/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3272 - auc: 0.9316 - val_loss: 0.3322 - val_auc: 0.9297\n",
            "Epoch 89/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3273 - auc: 0.9316 - val_loss: 0.3316 - val_auc: 0.9304\n",
            "Epoch 90/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3271 - auc: 0.9316 - val_loss: 0.3317 - val_auc: 0.9298\n",
            "Epoch 91/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3269 - auc: 0.9317 - val_loss: 0.3316 - val_auc: 0.9299\n",
            "Epoch 92/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3269 - auc: 0.9317 - val_loss: 0.3326 - val_auc: 0.9296\n",
            "Epoch 93/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3269 - auc: 0.9317 - val_loss: 0.3319 - val_auc: 0.9298\n",
            "Epoch 94/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3268 - auc: 0.9318 - val_loss: 0.3308 - val_auc: 0.9302\n",
            "Epoch 95/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3267 - auc: 0.9318 - val_loss: 0.3315 - val_auc: 0.9298\n",
            "Epoch 96/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3266 - auc: 0.9318 - val_loss: 0.3317 - val_auc: 0.9300\n",
            "Epoch 97/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3263 - auc: 0.9319 - val_loss: 0.3307 - val_auc: 0.9305\n",
            "Epoch 98/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3263 - auc: 0.9320 - val_loss: 0.3310 - val_auc: 0.9303\n",
            "Epoch 99/100\n",
            "2192/2192 [==============================] - 4s 2ms/step - loss: 0.3262 - auc: 0.9319 - val_loss: 0.3308 - val_auc: 0.9303\n",
            "Epoch 100/100\n",
            "2192/2192 [==============================] - 3s 2ms/step - loss: 0.3262 - auc: 0.9320 - val_loss: 0.3319 - val_auc: 0.9298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQXrmxvpXyeU"
      },
      "source": [
        "Calcoliamo la loss e l'accuracy sul test sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0R7-vC0kLqm",
        "outputId": "623d928b-54c1-4080-ad18-c0e6772e1003"
      },
      "source": [
        "test_loss, test_auc = model.evaluate(X_test, test_labels, verbose=2)\n",
        "print('Test AUC:', test_auc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2141/2141 - 2s - loss: 0.3287 - auc: 0.9305 - 2s/epoch - 1ms/step\n",
            "Test AUC: 0.9304797053337097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5SOkEGRXW7k",
        "outputId": "891cc66d-5fe5-4794-8a35-a2b365692a6c"
      },
      "source": [
        "#riassunto del modello\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 64)                1088      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                528       \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 8)                 136       \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 2)                 18        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,850\n",
            "Trainable params: 3,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "BNnUHxJNkbA2",
        "outputId": "a105a393-c3f6-4c44-83ea-82926a444ff1"
      },
      "source": [
        "# plot traing e validation loss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('val.jpeg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "2h6apIRtmdm1",
        "outputId": "1c318449-1bf2-4022-f4dc-2fb71e13813e"
      },
      "source": [
        "#plotting trainig e validation AUC\n",
        "\n",
        "acc_values = history_dict['auc']\n",
        "val_acc_values = history_dict['val_auc']\n",
        "\n",
        "plt.clf()\n",
        "plt.plot(epochs, acc_values, 'b', label='Training AUC')\n",
        "plt.plot(epochs, val_acc_values, 'orange', label='Validation AUC')\n",
        "plt.title('Training and validation AUC')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('AUC')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaVf8nmuYL9l"
      },
      "source": [
        "Utilizziamo la rete allenata per fare predizioni sul test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOVnNoAGm9k9",
        "outputId": "547a4bf5-1ae2-408b-cd83-29726e228ff1"
      },
      "source": [
        "predictions = model.predict(X_test)\n",
        "n_test=0\n",
        "\n",
        "print(predictions[n_test])\n",
        "print('Max probability for number:', np.argmax(predictions[n_test]))\n",
        "print(test_labels[n_test])\n",
        "print('True label is:',np.argmax(test_labels[n_test]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2141/2141 [==============================] - 2s 941us/step\n",
            "[0.16410357 0.83589643]\n",
            "Max probability for number: 1\n",
            "[0. 1.]\n",
            "True label is: 1\n"
          ]
        }
      ]
    }
  ]
}